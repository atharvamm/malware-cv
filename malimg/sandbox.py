import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
from PIL import Image


# %%
from numpy import load
import matplotlib.pylab as plt
import numpy as np
import os,glob,numpy
#import time

from matplotlib import image
from matplotlib import pyplot

import sys
import os
from math import log
import numpy as np
import scipy as sp
from PIL import Image

import os
import time
import sys
import yaml
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable


import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader


### Foolbox imports
#from foolbox.attacks import VirtualAdversarialAttack as vaa
# Load pre-trained model
#model.load_state_dict(torch.load('malware_classification_model.pth'))

checkpoints = torch.load("checkpoint.pth")
model.load_state_dict(checkpoints['model_state_dict'])

# Define input boundaries
lower_bound = torch.zeros((3, 224, 224))
upper_bound = torch.ones((3, 224, 224))

# Apply boundary attack
attack = BoundaryAttack(model, criterion=nn.CrossEntropyLoss(), lower_bound=lower_bound, upper_bound=upper_bound)
adv_samples = attack(val_loader) # Generate adversarial examples

# Evaluate model's robustness against the attack
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print('Accuracy on original test set: {:.2f}%'.format(accuracy))

    correct = 0
    total = 0
    for images, labels in adv_samples:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print('Accuracy on adversarial test set: {:.2f}%'.format(accuracy))



from evademl import CWAttack
from torchvision import transforms

# Load the pre-trained model


# Create a data transformation
data_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# Create the attack object
attack = CWAttack(model=model, 
                  targeted=False, 
                  max_iterations=1000, 
                  confidence=0.0, 
                  learning_rate=0.01)

# Generate adversarial examples for a batch of images
batch_size = 32
images, labels = ...  # Your input data
adversarial_images = []
for i in range(0, len(images), batch_size):
    batch_images = images[i:i+batch_size]
    batch_labels = labels[i:i+batch_size]
    adv_batch_images = attack(batch_images, batch_labels)
    adversarial_images.append(adv_batch_images)
adversarial_images = torch.cat(adversarial_images, dim=0)


from adversarial_robustness_toolbox.defenses.preprocessor import FeatureSqueezing
from adversarial_robustness_toolbox.utils import load_model, get_device

bit_depth = 2  # example bit depth, choose a value that suits your needs
preprocessor = FeatureSqueezing(bit_depth=bit_depth)

preprocessed_data = preprocessor(torch.tensor(x).to(device), model)
