
Project breakdown: 

Saranya will do a literature review and download the Malimg dataset, and set up the project environment.  - Done, March 19, downloaded from: https://sarvamblog.blogspot.com/2014/08/supervised-classification-with-k-fold.html

Saranya will work on replicating a SoTA ConvNet as a baseline. - MalConv - Done, March 19 - created my own CNN; i have malconv code (I think) but have not tested it yet 

Atharva will attack this ConvNet using prior work on adversarial attacks of malware image networks. : https://cs.paperswithcode.com/paper/adversarial-malware-binaries-evading-deep

These three tasks will make up the baseline that we hope to accomplish by the end of March. 

Our task is to improve the robustness of a malware image model. We hope to implement defensive distillation and pruning methods in March and April, hopefully experimenting on different architectures and methods starting next week. We will compare the precision/recall of our robustified network versus the original network on an adversarial dataset (either noisy Malimg data, or using prior work's adversarial data). 

Huy and Saranya will work on pruning methods, and Kriti and Atharva will work on defensive distillation. 

Saranya will do the writing of the final project, and Kriti and Huy will make the video. 


In this paper, we: 
\bit
\item {\bf Channel Prune} : We take each spatial filter in the model, zero out the least salient filters, and remove the corresponding row and columns. For any spatial filters that were zeroed and not pruned, we re-initialize them to their original values. 

\eit



\subsection{Pruning}


% Gradient-based saliency metrics work for pruning [Taxonomy of Saliency metrics for Channel Pruning]

% Channel pruning via automatic structure search 

% 2016 quantization paper 

% WeightWater [Mahoney] 

% Lottery ticket hypothesis 
%     see here for compression techniques 
    

% Barlow Twins https://arxiv.org/abs/2103.03230

% https://www.nature.com/articles/s41467-021-24025-8

There are two major branches of pruning, namely pruning after training (PaT), and pruning at initialization (PaI) \cite{wang2021recent}.
Typically PaT pruning is done in two steps: a large network is trained, and then channels are removed on the pre-trained network, and lastly the pruned network is fine-tuned. As PaT is structured, it can be used to accelerate deep models by sparsifying the structure, making it friendly to the hardware on mobile devices.

While PaT has been studied in depth [cite], PaI just became popular in recent years, largely in part to the Lottery Ticket Hypothesis. [cite] PaI finds the best sparse structure that achieves full accuracy by randomly initializing and training the networks. However, most of the PaI studies focus on unstructured pruning, with the goal of reducing the model size.


Layer-Wise Relevance Propagation, or LRP, is used to determine the relative contribution of different neurons. 
\cite{montavon2019layer}
LRP has a conservation property: relevance achieved by a neuron is redistributed to its lower layer in an equal amount:

$$R_j = \sum_k \frac{z_{jk}}{\Sigma_j z_{jk}}R_k$$

for a relevance score $R_k$ at a given layer and $z_{jk}$, the amount neuron $j$ contributes to make neuron $k$ relevant. In other words, even if individual neurons contribute differently, relevance across layers is conserved. Other terms can be appended to the LRP, including an $\epsilon$ that determines how sparse the neurons are, by eliminating weak or contradictory by absorbing their relevance.

We can then use relevance to spot weak or contradictory neurons that we can try to prune. \cite{voita2019analyzing} uses an LRP model to identify heads that are important to the model. 

LRP also helps with the robustness of models against phenomena like shattered gradients and adversarial examples. %https://iphome.hhi.de/samek/pdf/MonXAI19.pdf 
% https://www.frontiersin.org/articles/10.3389/fnagi.2019.00194/full

% 
% With the great advance of producing memory with high capacity in the last decade, we believe acceleration is more important when adopting deep models on mobile devices. %meaning?
% 

Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data

% https://arxiv.org/pdf/1805.12185.pdf 

% Pruning from Scratch  : https://www.semanticscholar.org/reader/454655f4e14e3892bfb574bd283ac5d4184847f4
PaT pruning, or pruning on a pre-trained model, is not the only way to prune post-hoc: instead, \cite{liu2018rethinking}  suggest pruning on randomized initial weights. They argue that trained models already contain a pruned structure that does not necessarily include the weights needed for a truly pruned model, as the weights of the pre-trained model tend to be homogeneous. Not only would this save compute power, as the model would not need to be trained, but the final pruned model has comparable performance compared to the pruned pre-trained model. 
Therefore, they use randomized weights and then learns channel importance via scalar gate values of each layer. Then, they binary search over all of the possible number of channels to determine a pruned structure without updating weights. 
